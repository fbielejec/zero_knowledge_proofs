* IN-PROGRESS Pre-Course
- https://leather-bulb-be5.notion.site/ZKP-Pre-Course-Self-Learn-Material-V2-31b9618231cd424fa72bdc5f11221bea
* DONE Linear algebra
** DONE vectors: https://www.youtube.com/watch?v=fNk_zzaMoSs

---

*NOTE: summary*

- geometric interpretation of vector addition and scalar multiplication

---


#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # Define vectors
  a = np.array([3, -5])
  b = np.array([2,1])

  # Add vectors
  result = a + b  # Element-wise addition
  print(f"{a}+{b}={result}")

  # multiply vector by a scalar
  scalar = 2
  print(f"{scalar}*{result}={scalar*result}")
#+END_SRC

#+RESULTS:
: [ 3 -5]+[2 1]=[ 5 -4]
: 2*[ 5 -4]=[10 -8]

** DONE Linear combinations, span, and basis vectors https://www.youtube.com/watch?v=k7RM-ot2NWY

---

*NOTE: summary*

- coordinates: two dimensional vectors
- coordinates can be thought of as pairs of scalars that scale $\hat{i}$ and $\hat{j}$ (the *basis* of the coordinate system).
- in xy coordinate system $\hat{i}$ is a special vector that points to the right with a length of 1 (unit vector in the x direction)
- $\hat{j}$ is a special vector that points up with a length 1 (unit vector in the y direction)
- x coordinates scales $\hat{i}$ and y coordinates scales $\hat{j}$.
- the vector that these coordinates describe is then the sum of the two scaled vectors $\hat{i}$ and $\hat{j}$:
  - e.g. $[3,-2] = (3) \cdot \hat{i} + (-2)\cdot \hat{j}$
- you can in fact arbitrarily choose any two different vectors as the basis and you get an entirely different coordinate system
- any time you scale and add two vectors like this it's called a *linear combination* of those two vectors:

=Definition: linear combination of vectors=

$a \cdot \vec{v} + b\cdot \vec{w}$

where:
- $a,b$ are scalars

=Definition: linearly dependent vectors=

One of the vectors can be expresses as a linear combination of the others:

$\vec{u} = a \cdot \vec{v} + b\cdot \vec{w}$

where:
- $a,b$ are some scalars (fixed values)

=Definition: linearly independent vectors=

One of the vectors cannot be expresses as a linear combination of the other(s):

$\forall a\in \mathbb{R} \hspace{1em} \vec{v} \neq a \cdot \vec{v}$
where:
 - the example is for 2 vectors but the same definition holds for 3 (or more) vectors.

- the *span* of two vectors is the set of all possible linear combinations of those vectors.
- 1 vector can describe all of 1d space (multiplied by a scalar can eventually span the whole space)
- adding a linearly independent vector to that one one unlocks another dimension
  - adding a linearly dependent one adds nothing new (no new dimension)
---

=Example: basis of the coordinate system=

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  np.random.seed(2137)

  # Define basis vectors
  i_hat = np.array([1,0])
  j_hat = np.array([0,1])

  v = np.array([3,-2])

  print(f"{v} = {v[0] * i_hat + v[1] * j_hat}")
#+END_SRC

#+RESULTS:
: [ 3 -2] = [ 3 -2]

=Example: span is a set of all linear combinations of a vector=

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  np.random.seed(2137)

def span(i, j):
    combinations = set()
    while len(combinations) < 10:
        a = random.randint(-10,10)
        b = random.randint(-10,10)
        combination = a*i + b *j
        combinations.add(tuple(combination))
    return combinations

# these two vectors are linearly dependent (they are a linear combination of one another)
i = np.array([1,0])
j = 2 * i
# notice how only one coordinate varies (only 1 dimension)
print(f"span of i={i} and j={j}: {span(i,j)}")

# these two vectors are linearly independent
i = np.array([1,0])
j = np.array([0,1])
# notice how both coordinates vary (we gain 1 dimension)
print(f"span of i={i} and j={j}: {span(i,j)}")
#+END_SRC

#+RESULTS:
: span of i=[1 0] and j=[2 0]: {(-8, 0), (4, 0), (-12, 0), (-17, 0), (-27, 0), (7, 0), (13, 0), (10, 0), (6, 0), (1, 0)}
: span of i=[1 0] and j=[0 1]: {(-9, -9), (4, 1), (2, -3), (1, 1), (3, 10), (-6, -3), (2, 9), (1, -3), (-8, -2), (-10, 8)}

** DONE Linear transformations and matrices: https://www.youtube.com/watch?v=kYB8IZa5AuE

---

*NOTE: chapter summary*

- transformations are functions with vectors as inputs that return vectors as outputs
- they act on the entire vector space
- linear transformations mean:
  - all lines must remain lines, without getting curved
  - origin of the coordinate system must remain in place
- the above property means that you need to only concern yourself with tracking the transformation of the origin, since every other vector fals in place (you can deuce where it lands based on where $\hat{i}$ and $\hat{j}$ land.
- 2D linear transformations is completely described by just four numbers, the two coords for where $\hat{i}$ lands and two coords where $\hat{j}$ lands.
- think of it this way: after applying certain transformation that morph the space in a linear way the vector $vec{v}$ lands on $vec{v_{transformed}}$ in this new space
  - but it can still be described in terms of the (now transformed) basis vectors

- *Formal linearity properties*
  - $L(\vec{v} + \vec{w})= L(\vec{v}) + L(\vec{w})$
  - $L(c\vec{v})= cL(\vec{v})$

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  # basis vectors
  i_hat = np.array([1,0])
  j_hat = np.array([0,1])

  v = -1*i_hat + 2*j_hat

  # applying some transformation:
  transformed_i = np.array([1,-2])
  transformed_j = np.array([3,0])

  transformed_v = -1* transformed_i + 2*transformed_j

  # matrix for the linear transformation:
  m = np.array([
                [transformed_i[0],transformed_j[0]],
                [transformed_i[1],transformed_j[1]]
               ])
  v = np.array([-1,2])

  # matrix times vector gives us the same transformed vector
  print(f"{transformed_v} == {m} * {v} == {np.matmul(m, v)}")
#+END_SRC

#+RESULTS:
: [5 2] == [[ 1  3]
:  [-2  0]] * [-1  2] == [5 2]

** DONE Matrix multiplication as composition https://www.youtube.com/watch?v=XkY2DOUCWMU

---

*NOTE: chapter summary*

- linear transformations can be combined (composed)
 - e.g. a rotation and a shear
- this combination is matrix multiplication
  - e.g. $A(BC) = (AB)C $
- matrix mulitplication is NOT commutative $A*B \neq B*A$

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  v = np.array([-1, 2])

  # counter-clockwise rotation by 90 deg
  # i_hat => [0,1]
  # j_hat => [-1,0]
  m1 = np.array([
                [0, -1],
                [1, 0]
               ])

  # a shear
  # i_hat => [1,0]
  # j_hat => [1,1]
  m2 = np.array([
                [1, 1],
                [0, 1]
               ])

  # composed tranformation has the same effect as applying two successive ones
  # reading m2 * m1 is right to left (first rotation than shear)
  print(f"{np.matmul(np.matmul(m2, m1),v)} == {np.matmul(m2, np.matmul(m1, v)) }")
#+END_SRC

#+RESULTS:
: [-3 -1] == [-3 -1]

** DONE 3D linear transformations: https://www.youtube.com/watch?v=rHLEWRxRGiM

---

*NOTE: chapter summary*

- basis vectors are now : $\hat{i}, \hat{j}, \hat{k}$
- applying a transformation is again done by multiplying the vectors of the space by the transformation matroiix, which consist of the transformed base vectors $\hat{i}, \hat{j}, \hat{k}$
  - m * v = v[0] * m[,0] +  v[1] * m[,1] +  v[2] * m[,2]

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  # basis
  i = np.array([1,0,-1])x
  j = np.array([1,1,0])
  k = np.array([1,0,1])

  m = np.array([
                [i[0], j[0], k[0]],
                [i[1], j[1], k[1]],
                [i[2], j[2], k[2]]
               ])

  v = np.array([3,2,1])



  print(f"{m}")
#+END_SRC

#+RESULTS:
: [[ 1  1  1]
:  [ 0  1  0]
:  [-1  0  1]]

** DONE The determinant: https://www.youtube.com/watch?v=Ip3X9LOh2dk

---

*NOTE: chapter summary*

- Linear transformations stretch or squish the space
- The determinant measures how much volumes change during a transformation (increase or decrease)
  - e.g. in a case of a linear transformation $3*\hat{i}$ and $2*\hat{i}$ a square on the space grid that was 1:1 is now 2:3
  - if the area was A it is now 6*A
- det(M) where M is the transformation matrix is the measure of that increase (or decrease in the area)
- It is also possible that the area after applying a transformation does not change at all.
  - In that case $det(M)=1$
- It is even possible for a transformation to squish the entire space into a single line, or a point ($det(M)=0$).
- Negative determinant means that the transformation inverts the space
 - it's absolute value will still tell you the factor of the area increase (or decrease)
- In case of 3 dimensions we can talk about the volume increasing or decreasing
- computing the determinant:
$det(\begin{bmatrix}
a & b \\
c & d
\end{bmatrix})=ad-bc$

- $det(M_1 \cdot M_2) = det(M_1) \cdot det(M_2)$

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  # transformed basis vectors
  i_hat = 3 * np.array([1,0]) # i scaled by factor of 3
  j_hat = 2 * np.array([0,1]) # j scaled by factor of 2

  # transformation matrix
  m = np.array([
                [i_hat[0],j_hat[0]],
                [i_hat[1],j_hat[1]]
               ])

  print(f"det({m}) = {np.linalg.det(m)}")
#+END_SRC

#+RESULTS:
: det([[3 0]
:  [0 2]]) = 6.0

** DONE Inverse matrices, column space, rank and null space: https://www.youtube.com/watch?v=uQhTuRlWMxw

---

*NOTE: chapter summary*

- solving linear systems of equations
- keywords:
  - Gaussian elimination
  - raw echelon form

- Example: a linear system of equations:

$\begin{cases}
2x + 5y +3z=-3 \\
4x+0y+8z = 0 \\
1x + 3y + 0z = 2
\end{cases}$

- Same system in a matrix form:

$\begin{bmatrix}
2 & 5 & 3 \\
4 & 0 & 8 \\
1 & 3 & 0
\end{bmatrix} \cdot \begin{bmatrix} x\\ y\\ z \end{bmatrix} = \begin{bmatrix} -3\\ 0\\ 2 \end{bmatrix}$

- A $\cdot \vec{x} = \vec{v}$
- where:
  - $A$ corresponds to some linear transformation, therefore the equations means looking for a vector $\vec{x}$ which after applying $A$ lands on $\vec{v}$

- *Determinant*
- We can check the determinant to conclude whether the system has a solution:
- Case 1: $det(A) \neq 0$
  - this means the transformation preserves all the dimensions and there is a unique solution.
  - In this case solving for $\vec{x}$ is akin to reversing the linear transformation applied by $A$:
  - $x= A^{-1} \times \vec{v}$
  - where $A^{-1}$ (inverse of $A$) is basically a different linear transformation (a transformation which reverses transformation $A$).
  - Applying $A$ and then inversing it: $A\cdot A^{-1}=I$ gets us right back to where we started (a transformation that does nothing, an identity matrix).
- Case 2: $det(A) = 0$
  - the transformation squishes the space into a smaller dimension, there is no inverse
  - there can still be a solution (if the vector $\vec{v}$ "lives" on the line that the space is squished to)

- *Rank: number of dimensions in the transformed space*
  - when the output of the transformation is a line (it's one dimensional) we say that transformation $A$ has a rank of 1.
  - if all the vectors land on a plane it has a rank of 2.

- *Column space*
 - the set of all possible outputs $A\vec{v}$
 - matrix is full rank if the rank is as high as it can be (i.e. equals the number of columns of that matrix)

- *Null space (matrix kernel)*
  - what the set of all the solutions looks like

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # counter-clockwise rotation
  a = np.array([[0,1],
                [-1,0]])

  # clockwise rotation
  a_inv = np.linalg.inv(a)

  print(f"{a}^-1 = {a_inv}")

  # this is the identity matrix, it leaves i_hat and j_hat where they started
  print(f"a^-1 a = {np.matmul(a, a_inv)}")

#+END_SRC

#+RESULTS:
: [[ 0  1]
:  [-1  0]]^-1 = [[-0. -1.]
:  [ 1.  0.]]
: a^-1 a = [[1. 0.]
:  [0. 1.]]

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  #from sympy import symbols, solve, Eq, lambdify

  a = np.array([[2,2],
                [1,3]])

  #x = np.array([symbols('x'),symbols('y')])

  v = np.array([-4,-1])

  # positive determinant
  print(f"det({a}) = {np.linalg.det(a)}")

  # x = A^{-1} * v
  a_inv = np.linalg.inv(a)
  x = np.matmul(a_inv,v)
  print(f"x = {x}")

#+END_SRC

#+RESULTS:
: det([[2 2]
:  [1 3]]) = 4.0
: x = [-2.5  0.5]

** DONE Nonsquare matrices as transformations between dimensions: https://www.youtube.com/watch?v=v8VSDg_WQlA

---

*NOTE: chapter summary*

- transformations between dimension such as ones that take 2D vectors to 3D vectors
 - such transformations are still linear (origin maps to the origin and the gridlines remain parallel and evenly spaced)
- a transformation that takes a 2D space to a 1D (a line) is also perfectly possible
  - e.g. $\begin{bmatrix} 2 & 1 \end{bmatrix}$ means that $\hat{i}$ lands on 2 and $\hat{j}$ lands on 1

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # output of a transformation that takes i to:
  transformed_i = np.array([2,-1,-2])
  # and j to:
  transformed_j = np.array([0,1,1])

  # 3 x 2 matrix
  a = np.array([[transformed_i[0], transformed_j[0]],
                [transformed_i[1], transformed_j[1]],
                [transformed_i[2], transformed_j[2]]])

#+END_SRC

#+RESULTS:

** DONE Dot products and duality: https://www.youtube.com/watch?v=LyGKycYT2v0

---

*NOTE: chapter summary*

- matrix-vector product <=> dot product

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # original vector in 2D space
  v = np.array([[4], [3]])

  # a transformation that takes 2D to 1D (i to 1, j to -2)
  a = np.array([1, -2])

  v_transformed = np.matmul(a, v)
  print(f"L({v}) = {v_transformed}")
#+END_SRC

#+RESULTS:
: L([[4]
:  [3]]) = [-2]

* DONE Practice Problems
** DONE Exercise 1

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  A = [[1,2,3],[4,5,6],[7,8,9]]
  B = [[1,1,1],[2,2,2],[3,3,3]]
  C = [[2,3,4],[6,7,8],[10,11,12]]

  def add_arrays(A, B):
      return np.array(A) + np.array(B)

  assert (add_arrays(A, B) == np.array(C)).all()

  # .all = element-wise equlity check (every element in the array is true)
  print(np.array([True, True, True]).all())
  print(np.array([False, True, True]).all())
#+END_SRC

#+RESULTS:
: True
: False

** DONE Exercise 2: matrix multiplication

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def matrix_multiply(A, B):
      return np.matmul(A, B)

  A = np.array([[0,1],
                [-1,0]])

  B = np.array([[-0., -1.],
                [ 1.,  0.]])

  C = np.array([[1., 0.],
                [0., 1.]])

  assert (matrix_multiply(A, B) == C).all()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def element_wise_multiply(A, B):
      return np.multiply(A,B)

  A  = np.array([[0,1],
                 [-1,0]])

  B = np.array([[-0., -1.],
                [ 1.,  0.]])

  C = np.array([[0., -1.],
                [-1, 0.]])

  assert (element_wise_multiply(A, B) == C).all()
#+END_SRC

#+RESULTS:

** DONE Exercise 3: dot product

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def dot_product(A, B):
      return A @ B

  A = np.array([1,2,3,4])
  B = np.array([[1],[2],[3],[4]])

  assert (dot_product(A, B) == 30)
#+END_SRC

#+RESULTS:

** DONE Exercise 4: linear combinations

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def linearCombination(A, B, a, b):
      # Todo, add your code
      return a*A + b*B

  vector1 = np.array([1,2])
  vector2 = np.array([5,6])
  scalar1 = 3
  scalar2 = 10

  assert (np.array([53, 66]) == linearCombination(vector1, vector2, scalar1, scalar2)).all()
#+END_SRC

#+RESULTS:

** DONE Exercise 5: modular arithmetic

- The challenge here is to compute the modular inverse of 15 % 1223.
- That is 15 * x % 1223 == 1.

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  a=15;m=1223;

  # multiplicative inverse exists (a and m are coprime)
  print(f"GCD({a},{m})={np.gcd(a,m)}")

  # GCD (48,18) by euclidean algorithm
  48 % 18
  18 % 12
  12 % 6 # gcd(48,18) = 6

  def gcd(a, b):
      if b == 0:
          return a
      return gcd(b, a % b)

  assert(gcd(a,m) == 1)

  a=48;b=18;

  def gcdExtended(a, b):
      # Base Case
      if a == 0 :
          return b,0,1

      gcd,x1,y1 = gcdExtended(b%a, a)

      # Update x and y using results of recursive the call
      x = y1 - (b//a) * x1
      y = x1

      return gcd,x,y

  # example15 moonmath
  assert(gcdExtended(6,5) == (1,1,-1))

  # 15 * x % 1223 == 1 <=>
  # 15 * x1 + 1223 * y1 == 1
  gcd,x1,y1 = gcdExtended(a,m)

  print(f"{a*x1 + m*y1} == {a} * {x1} + {m} * {y1} == {gcd}")
  print(f"{a}^-1 * {a} mod {m} = {(x1 * a) % m} => {a}^-1 mod {m} = {x1}")

  assert a * x1 % m == 1
#+END_SRC

#+RESULTS:
: GCD(15,1223)=1
: 1 == 48 * -586 + 1223 * 23 == 1
: 48^-1 * 48 mod 1223 = 1 => 48^-1 mod 1223 = -586

** DONE Exercise 6: Column and Row Slicing

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

def get_column_as_1d(A, col_number):
    return A[:, col_number]

def get_row_as_1d(A, row_number):
    return A[row_number, :]

A = np.array([[1,2,3],[4,5,6],[7,8,9]])

# [2,5,8]
print(get_column_as_1d(A, 1))

print(get_row_as_1d(A, 2)) # [7,8,9]
#+END_SRC

#+RESULTS:
: [2 5 8]
: [7 8 9]

* DONE P vs NP Problem
- *P (Polynomial) Problems:* problems that can be solved in polynomial time and whose solutions can be verified in polynomial time are called *problems in P*.
  - e.g. Sorting a list : e.g. mergesort takes $\mathcal{O}(n \cdot log(n))< \mathcal{O}^{2}$ to sort a list and $\mathcal{O}(n)$ to verify that the list is sorted.
- *The Witness*: proof that you solved a problem correctly
  - in the example above a sorted list would be a witness
- *NP (non-deterministic polynomial) Problems:*
  - Any problem whose proposed solution (a *witness*) can be quickly verified as correct is a problem in NP.
  - However, finding the solution might require exponential resources.
  - Example: Computing the solution to a Sudoku puzzle and verifying the proposed solution to a Sudoku puzzle.
    - we can quickly verify the solution is correct simply by looping over the columns, rows, and subgrids, meaning the witness can be verified in polynomial time.
    - computing the solution requires significantly more resources: $\mathcal{O}(c^n)$, where $n$ is the number of rows == number of columns in the puzzle
  - Example: 3-coloring a map: https://www.youtube.com/watch?v=WlcXoz6tn4g
- *Problems in PSPACE*
  - Problems that require exponential resources to solve and verify
- *P vs NP*
  - P is the class of problems that can be solved and verified efficiently
  - NP is the class of problems that can be verified efficiently.
  - are these two classes of the problems the same?
  - Finding a solution is always *at least as hard as verifying it*. (By definition, solving a problem includes finding the correct answer, which means an algorithm that solves the problem is also verifying its answer in the process)
  - If P = NP, it would mean that whenever we can find an efficient method for verifying a solution, we can also find an efficient method for finding that solution.
  - researchers widely speculate that $P \neq NP$ and $NP \neq PSPACE$, no formal mathematical proof exists for these conclusions.
* IN-PROGRESS Expressing problems and solutions as Boolean formulas

- All problems in P and NP can be verified by transforming them to *boolean formulas*.
- sage: operators &, |, ~, ^ corresponding to and, or, not, xor
  - also existing ->, <-> : if...then, if and only if.

#+BEGIN_SRC sage :session . :exports both
  import sage.logic.propcalc as propcalc

  f = propcalc.formula("(x1 | ~x2 | ~x3) & (~x2 | x3 | x4) & (~x2 | ~x3 | ~x4)")
  f.evaluate({'x1' : True, 'x2' : False, 'x3' : True, 'x4' : False})
#+END_SRC

#+RESULTS:
: True

- For example
  - $A > B$ can be modeled with $A \land \neg B$
  - $A = B$ can be modeled with $(A \land B) \lor \neg (A \lor B)$

- Checking whether A > B (in 4 bit logic)

#+BEGIN_SRC jupyter-python :session zk :async yes
  A=11
  B=9

  def bits(n, endian='little'):
      bits_le = [(n >> i) & 1 for i in range(4)]
      if endian == 'big':
          return bits_le[::-1]
      return bits_le

  bits_be_11 = bits(A, endian = 'big')
  print(f"11 = {bits_be_11}")

  bits_be_9 = bits(B, endian = 'big')
  print(f"9 = {bits_be_9}")

  # Binary comparison by most significant different bit
  def compare(bits_p, bits_q):
      """
      Compare integers P and Q bit decomposition
      Assumes same number of bits!

      Returns:
        0 if P == Q, 1 id P > Q, -1 if P < Q
      """
      for idx in range(len(bits_p)):
          p = bits_p[idx]
          q = bits_q[idx]
          if p != q:
              return p - q
      return 0

  compare(bits_be_11, bits_be_9)
#+END_SRC

#+RESULTS:
:RESULTS:
: 11 = [1, 0, 1, 1]
: 9 = [1, 0, 0, 1]
: 1
:END:

- checking if $P \geq  Q$ using a boolean formula:
  - bits in P are $[p_{4}, p_{3}, p_{2}, p_{1}]$ (ordered MSB to LSB)
  - bits in Q are $[q_{4}, q_{3}, q_{2}, q_{1}]$

$\begin{cases}
 (p_{4} > q_{4}) \hspace{.5em}  \lor \\
((p_{4} = q_{4}) \land (p_{3} > q_{3})) \hspace{.5em} \lor \\
((p_{4} = q_{4}) \land (p_{3} = q_{3}) \land (p_{2} > q_{2}) ) \hspace{.5em} \lor \\
((p_{4} = q_{4}) \land (p_{3} = q_{3}) \land (p_{2} = q_{2}) \land ((p_{1} > q_{1}) \lor (p_{1} = q_{1}) ) )
\end{cases}$

#+BEGIN_SRC sage :session . :exports both
  import sage.logic.propcalc as propcalc

  # bit p > q
  def p_geq_q(bit_p, bit_q):  
      return propcalc.formula("p{bit_p} & ~q{bit_q}".format(bit_p = bit_p, bit_q = bit_q))

  # bit p == q
  def p_eq_q(bit_p, bit_q):  
      return propcalc.formula("(p{bit_p} & q{bit_q}) | ~(p{bit_p} | q{bit_q})".format(bit_p = bit_p, bit_q = bit_q))

  witness = {'p4' : True, 'p3' : False, 'p2' : True, 'p1' : True,
             'q4' : True, 'q3' : False, 'q2' : False, 'q1' : True}

  # I know a witness (decomposed P and Q) such that P >= Q
  p_geq_q(4,4).evaluate(witness) | (p_eq_q(4,4).evaluate(witness) & p_geq_q(3,3).evaluate(witness)) | (p_eq_q(4,4).evaluate(witness) & p_eq_q(3,3).evaluate(witness) & p_geq_q(2,2).evaluate(witness)) | (p_eq_q(4,4).evaluate(witness) & p_eq_q(3,3).evaluate(witness) & p_eq_q(2,2).evaluate(witness) & (p_geq_q(1,1).evaluate(witness) | p_eq_q(1,1).evaluate(witness))) 
   
#+END_SRC

#+RESULTS:
: True

- You can *check if a list is sorted* using such an expression:
  - repeatedly apply the above comparison expression to each pair of adjacent elements in the list and combine the comparison expressions using the *AND* operation.

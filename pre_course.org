* IN-PROGRESS Pre-Course
- https://leather-bulb-be5.notion.site/ZKP-Pre-Course-Self-Learn-Material-V2-31b9618231cd424fa72bdc5f11221bea
* IN-PROGRESS Linear algebra
** DONE vectors: https://www.youtube.com/watch?v=fNk_zzaMoSs

---

*NOTE: summary*

- geometric interpretation of vector addition and scalar multiplication

---


#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # Define vectors
  a = np.array([3, -5])
  b = np.array([2,1])

  # Add vectors
  result = a + b  # Element-wise addition
  print(f"{a}+{b}={result}")

  # multiply vector by a scalar
  scalar = 2
  print(f"{scalar}*{result}={scalar*result}")
#+END_SRC

#+RESULTS:
: [ 3 -5]+[2 1]=[ 5 -4]
: 2*[ 5 -4]=[10 -8]

** DONE Linear combinations, span, and basis vectors https://www.youtube.com/watch?v=k7RM-ot2NWY

---

*NOTE: summary*

- coordinates: two dimensional vectors
- coordinates can be thought of as pairs of scalars that scale $\hat{i}$ and $\hat{j}$ (the *basis* of the coordinate system).
- in xy coordinate system $\hat{i}$ is a special vector that points to the right with a length of 1 (unit vector in the x direction)
- $\hat{j}$ is a special vector that points up with a length 1 (unit vector in the y direction)
- x coordinates scales $\hat{i}$ and y coordinates scales $\hat{j}$.
- the vector that these coordinates describe is then the sum of the two scaled vectors $\hat{i}$ and $\hat{j}$:
  - e.g. $[3,-2] = (3) \cdot \hat{i} + (-2)\cdot \hat{j}$
- you can in fact arbitrarily choose any two different vectors as the basis and you get an entirely different coordinate system
- any time you scale and add two vectors like this it's called a *linear combination* of those two vectors:

=Definition: linear combination of vectors=

$a \cdot \vec{v} + b\cdot \vec{w}$

where:
- $a,b$ are scalars

=Definition: linearly dependent vectors=

One of the vectors can be expresses as a linear combination of the others:

$\vec{u} = a \cdot \vec{v} + b\cdot \vec{w}$

where:
- $a,b$ are some scalars (fixed values)

=Definition: linearly independent vectors=

One of the vectors cannot be expresses as a linear combination of the other(s):

$\forall a\in \mathbb{R} \hspace{1em} \vec{v} \neq a \cdot \vec{v}$
where:
 - the example is for 2 vectors but the same definition holds for 3 (or more) vectors.

- the *span* of two vectors is the set of all possible linear combinations of those vectors.
- 1 vector can describe all of 1d space (multiplied by a scalar can eventually span the whole space)
- adding a linearly independent vector to that one one unlocks another dimension
  - adding a linearly dependent one adds nothing new (no new dimension)
---

=Example: basis of the coordinate system=

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  np.random.seed(2137)

  # Define basis vectors
  i_hat = np.array([1,0])
  j_hat = np.array([0,1])

  v = np.array([3,-2])

  print(f"{v} = {v[0] * i_hat + v[1] * j_hat}")
#+END_SRC

#+RESULTS:
: [ 3 -2] = [ 3 -2]

=Example: span is a set of all linear combinations of a vector=

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  np.random.seed(2137)

def span(i, j):
    combinations = set()
    while len(combinations) < 10:
        a = random.randint(-10,10)
        b = random.randint(-10,10)
        combination = a*i + b *j
        combinations.add(tuple(combination))
    return combinations

# these two vectors are linearly dependent (they are a linear combination of one another)
i = np.array([1,0])
j = 2 * i
# notice how only one coordinate varies (only 1 dimension)
print(f"span of i={i} and j={j}: {span(i,j)}")

# these two vectors are linearly independent
i = np.array([1,0])
j = np.array([0,1])
# notice how both coordinates vary (we gain 1 dimension)
print(f"span of i={i} and j={j}: {span(i,j)}")
#+END_SRC

#+RESULTS:
: span of i=[1 0] and j=[2 0]: {(-8, 0), (4, 0), (-12, 0), (-17, 0), (-27, 0), (7, 0), (13, 0), (10, 0), (6, 0), (1, 0)}
: span of i=[1 0] and j=[0 1]: {(-9, -9), (4, 1), (2, -3), (1, 1), (3, 10), (-6, -3), (2, 9), (1, -3), (-8, -2), (-10, 8)}

** DONE Linear transformations and matrices: https://www.youtube.com/watch?v=kYB8IZa5AuE

---

*NOTE: chapter summary*

- transformations are functions with vectors as inputs that return vectors as outputs
- they act on the entire vector space
- linear transformations mean:
  - all lines must remain lines, without getting curved
  - origin of the coordinate system must remain in place
- the above property means that you need to only concern yourself with tracking the transformation of the origin, since every other vector fals in place (you can deuce where it lands based on where $\hat{i}$ and $\hat{j}$ land.
- 2D linear transformations is completely described by just four numbers, the two coords for where $\hat{i}$ lands and two coords where $\hat{j}$ lands.
- think of it this way: after applying certain transformation that morph the space in a linear way the vector $vec{v}$ lands on $vec{v_{transformed}}$ in this new space
  - but it can still be described in terms of the (now transformed) basis vectors

- *Formal linearity properties*
  - $L(\vec{v} + \vec{w})= L(\vec{v}) + L(\vec{w})$
  - $L(c\vec{v})= cL(\vec{v})$

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  # basis vectors
  i_hat = np.array([1,0])
  j_hat = np.array([0,1])

  v = -1*i_hat + 2*j_hat

  # applying some transformation:
  transformed_i = np.array([1,-2])
  transformed_j = np.array([3,0])

  transformed_v = -1* transformed_i + 2*transformed_j

  # matrix for the linear transformation:
  m = np.array([
                [transformed_i[0],transformed_j[0]],
                [transformed_i[1],transformed_j[1]]
               ])
  v = np.array([-1,2])

  # matrix times vector gives us the same transformed vector
  print(f"{transformed_v} == {m} * {v} == {np.matmul(m, v)}")
#+END_SRC

#+RESULTS:
: [5 2] == [[ 1  3]
:  [-2  0]] * [-1  2] == [5 2]

** DONE Matrix multiplication as composition https://www.youtube.com/watch?v=XkY2DOUCWMU

---

*NOTE: chapter summary*

- linear transformations can be combined (composed)
 - e.g. a rotation and a shear
- this combination is matrix multiplication
  - e.g. $A(BC) = (AB)C $
- matrix mulitplication is NOT commutative $A*B \neq B*A$

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  v = np.array([-1, 2])

  # counter-clockwise rotation by 90 deg
  # i_hat => [0,1]
  # j_hat => [-1,0]
  m1 = np.array([
                [0, -1],
                [1, 0]
               ])

  # a shear
  # i_hat => [1,0]
  # j_hat => [1,1]
  m2 = np.array([
                [1, 1],
                [0, 1]
               ])

  # composed tranformation has the same effect as applying two successive ones
  # reading m2 * m1 is right to left (first rotation than shear)
  print(f"{np.matmul(np.matmul(m2, m1),v)} == {np.matmul(m2, np.matmul(m1, v)) }")
#+END_SRC

#+RESULTS:
: [-3 -1] == [-3 -1]

** DONE 3D linear transformations: https://www.youtube.com/watch?v=rHLEWRxRGiM

---

*NOTE: chapter summary*

- basis vectors are now : $\hat{i}, \hat{j}, \hat{k}$
- applying a transformation is again done by multiplying the vectors of the space by the transformation matroiix, which consist of the transformed base vectors $\hat{i}, \hat{j}, \hat{k}$
  - m * v = v[0] * m[,0] +  v[1] * m[,1] +  v[2] * m[,2]

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  # basis
  i = np.array([1,0,-1])x
  j = np.array([1,1,0])
  k = np.array([1,0,1])

  m = np.array([
                [i[0], j[0], k[0]],
                [i[1], j[1], k[1]],
                [i[2], j[2], k[2]]
               ])

  v = np.array([3,2,1])



  print(f"{m}")
#+END_SRC

#+RESULTS:
: [[ 1  1  1]
:  [ 0  1  0]
:  [-1  0  1]]

** DONE The determinant: https://www.youtube.com/watch?v=Ip3X9LOh2dk

---

*NOTE: chapter summary*

- Linear transformations stretch or squish the space
- The determinant measures how much volumes change during a transformation (increase or decrease)
  - e.g. in a case of a linear transformation $3*\hat{i}$ and $2*\hat{i}$ a square on the space grid that was 1:1 is now 2:3
  - if the area was A it is now 6*A
- det(M) where M is the transformation matrix is the measure of that increase (or decrease in the area)
- It is also possible that the area after applying a transformation does not change at all.
  - In that case $det(M)=1$
- It is even possible for a transformation to squish the entire space into a single line, or a point ($det(M)=0$).
- Negative determinant means that the transformation inverts the space
 - it's absolute value will still tell you the factor of the area increase (or decrease)
- In case of 3 dimensions we can talk about the volume increasing or decreasing
- computing the determinant:
$det(\begin{bmatrix}
a & b \\
c & d
\end{bmatrix})=ad-bc$

- $det(M_1 \cdot M_2) = det(M_1) \cdot det(M_2)$

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  import random

  # transformed basis vectors
  i_hat = 3 * np.array([1,0]) # i scaled by factor of 3
  j_hat = 2 * np.array([0,1]) # j scaled by factor of 2

  # transformation matrix
  m = np.array([
                [i_hat[0],j_hat[0]],
                [i_hat[1],j_hat[1]]
               ])

  print(f"det({m}) = {np.linalg.det(m)}")
#+END_SRC

#+RESULTS:
: det([[3 0]
:  [0 2]]) = 6.0

** DONE Inverse matrices, column space, rank and null space: https://www.youtube.com/watch?v=uQhTuRlWMxw

---

*NOTE: chapter summary*

- solving linear systems of equations
- keywords:
  - Gaussian elimination
  - raw echelon form

- Example: a linear system of equations:

$\begin{cases}
2x + 5y +3z=-3 \\
4x+0y+8z = 0 \\
1x + 3y + 0z = 2
\end{cases}$

- Same system in a matrix form:

$\begin{bmatrix}
2 & 5 & 3 \\
4 & 0 & 8 \\
1 & 3 & 0
\end{bmatrix} \cdot \begin{bmatrix} x\\ y\\ z \end{bmatrix} = \begin{bmatrix} -3\\ 0\\ 2 \end{bmatrix}$

- A $\cdot \vec{x} = \vec{v}$
- where:
  - $A$ corresponds to some linear transformation, therefore the equations means looking for a vector $\vec{x}$ which after applying $A$ lands on $\vec{v}$

- *Determinant*
- We can check the determinant to conclude whether the system has a solution:
- Case 1: $det(A) \neq 0$
  - this means the transformation preserves all the dimensions and there is a unique solution.
  - In this case solving for $\vec{x}$ is akin to reversing the linear transformation applied by $A$:
  - $x= A^{-1} \times \vec{v}$
  - where $A^{-1}$ (inverse of $A$) is basically a different linear transformation (a transformation which reverses transformation $A$).
  - Applying $A$ and then inversing it: $A\cdot A^{-1}=I$ gets us right back to where we started (a transformation that does nothing, an identity matrix).
- Case 2: $det(A) = 0$
  - the transformation squishes the space into a smaller dimension, there is no inverse
  - there can still be a solution (if the vector $\vec{v}$ "lives" on the line that the space is squished to)

- *Rank: number of dimensions in the transformed space*
  - when the output of the transformation is a line (it's one dimensional) we say that transformation $A$ has a rank of 1.
  - if all the vectors land on a plane it has a rank of 2.

- *Column space*
 - the set of all possible outputs $A\vec{v}$
 - matrix is full rank if the rank is as high as it can be (i.e. equals the number of columns of that matrix)

- *Null space (matrix kernel)*
  - what the set of all the solutions looks like

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # counter-clockwise rotation
  a = np.array([[0,1],
                [-1,0]])

  # clockwise rotation
  a_inv = np.linalg.inv(a)

  print(f"{a}^-1 = {a_inv}")

  # this is the identity matrix, it leaves i_hat and j_hat where they started
  print(f"a^-1 a = {np.matmul(a, a_inv)}")

#+END_SRC

#+RESULTS:
: [[ 0  1]
:  [-1  0]]^-1 = [[-0. -1.]
:  [ 1.  0.]]
: a^-1 a = [[1. 0.]
:  [0. 1.]]

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np
  #from sympy import symbols, solve, Eq, lambdify

  a = np.array([[2,2],
                [1,3]])

  #x = np.array([symbols('x'),symbols('y')])

  v = np.array([-4,-1])

  # positive determinant
  print(f"det({a}) = {np.linalg.det(a)}")

  # x = A^{-1} * v
  a_inv = np.linalg.inv(a)
  x = np.matmul(a_inv,v)
  print(f"x = {x}")

#+END_SRC

#+RESULTS:
: det([[2 2]
:  [1 3]]) = 4.0
: x = [-2.5  0.5]

** DONE Nonsquare matrices as transformations between dimensions: https://www.youtube.com/watch?v=v8VSDg_WQlA

---

*NOTE: chapter summary*

- transformations between dimension such as ones that take 2D vectors to 3D vectors
 - such transformations are still linear (origin maps to the origin and the gridlines remain parallel and evenly spaced)
- a transformation that takes a 2D space to a 1D (a line) is also perfectly possible
  - e.g. $\begin{bmatrix} 2 & 1 \end{bmatrix}$ means that $\hat{i}$ lands on 2 and $\hat{j}$ lands on 1

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # output of a transformation that takes i to:
  transformed_i = np.array([2,-1,-2])
  # and j to:
  transformed_j = np.array([0,1,1])

  # 3 x 2 matrix
  a = np.array([[transformed_i[0], transformed_j[0]],
                [transformed_i[1], transformed_j[1]],
                [transformed_i[2], transformed_j[2]]])

#+END_SRC

#+RESULTS:

** DONE Dot products and duality: https://www.youtube.com/watch?v=LyGKycYT2v0

---

*NOTE: chapter summary*

- matrix-vector product <=> dot product

---

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  # original vector in 2D space
  v = np.array([[4], [3]])

  # a transformation that takes 2D to 1D (i to 1, j to -2)
  a = np.array([1, -2])

  v_transformed = np.matmul(a, v)
  print(f"L({v}) = {v_transformed}")
#+END_SRC

#+RESULTS:
: L([[4]
:  [3]]) = [-2]

* IN-PROGRESS Practice Problems
** DONE Exercise 1
#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  A = [[1,2,3],[4,5,6],[7,8,9]]
  B = [[1,1,1],[2,2,2],[3,3,3]]
  C = [[2,3,4],[6,7,8],[10,11,12]]

  def add_arrays(A, B):
      return np.array(A) + np.array(B)

  assert (add_arrays(A, B) == np.array(C)).all()

  # .all = element-wise equlity check (every element in the array is true)
  print(np.array([True, True, True]).all())
  print(np.array([False, True, True]).all())
#+END_SRC

#+RESULTS:
: True
: False

** DONE Exercise 2
#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def matrix_multiply(A, B):
      return np.matmul(A, B)

  A = np.array([[0,1],
                [-1,0]])

  B = np.array([[-0., -1.],
                [ 1.,  0.]])

  C = np.array([[1., 0.],
                [0., 1.]])

  assert (matrix_multiply(A, B) == C).all()
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def element_wise_multiply(A, B):
      return np.multiply(A,B)

  A  = np.array([[0,1],
                 [-1,0]])

  B = np.array([[-0., -1.],
                [ 1.,  0.]])

  C = np.array([[0., -1.],
                [-1, 0.]])

  assert (element_wise_multiply(A, B) == C).all()
#+END_SRC

#+RESULTS:

** DONE Exercise 3
#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np

  def dot_product(A, B):
      return A @ B

  A = np.array([1,2,3,4])
  B = np.array([[1],[2],[3],[4]])
 
  assert (dot_product(A, B) == 30)
#+END_SRC

#+RESULTS:

** IN-PROGRESS Exercise 4
#+BEGIN_SRC jupyter-python :session zk :async yes
  import numpy as np


#+END_SRC
